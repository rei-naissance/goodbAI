{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxscript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAtUc38H4RAD",
        "outputId": "ad445987-3970-4fc1-d082-58f0b4c1a55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\n",
            "Requirement already satisfied: onnx_ir<2,>=0.1.15 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.1.16)\n",
            "Requirement already satisfied: onnx>=1.17 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (26.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17->onnxscript) (5.29.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "V7lJ_cAZx7gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile convert_to_onnx.py\n",
        "import torch\n",
        "from torch import nn\n",
        "from sonics import HFAudioClassifier\n",
        "import torchlibrosa as tl\n",
        "from fire import Fire\n",
        "import librosa\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "ORIGINAL_SR = 44100\n",
        "TARGET_SR = 16000\n",
        "\n",
        "def convert_to_onnx(\n",
        "    model_id=\"awsaf49/sonics-spectttra-alpha-5s\",\n",
        "    output_path=\"sonics_model.onnx\",\n",
        "):\n",
        "    print(f\"Converting model {model_id}...\")\n",
        "    print(f\"Downloading model from HuggingFace...\")\n",
        "    model = HFAudioClassifierProb.from_pretrained(model_id)\n",
        "    model.eval()\n",
        "    model = replace_melspec(model)\n",
        "    max_time = model.config.audio.max_time\n",
        "    dummy_input = torch.randn(1, ORIGINAL_SR * max_time)\n",
        "    print(f\"Exporting to onnx and saving to {output_path}...\")\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        output_path,\n",
        "        input_names=[\"audio\"],\n",
        "        output_names=[\"prob\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        "        dynamo=False,\n",
        "    )\n",
        "    print(\"Done!\")\n",
        "\n",
        "def replace_melspec(model):\n",
        "    model.ft_extractor.audio2melspec = Audio2MelspecTL(\n",
        "        n_fft=model.config.melspec.n_fft,\n",
        "        hop_length=model.config.melspec.hop_length,\n",
        "        win_length=model.config.melspec.win_length,\n",
        "        power=model.config.melspec.power,\n",
        "        sr=model.config.audio.sample_rate,\n",
        "        n_mels=model.config.melspec.n_mels,\n",
        "        fmin=model.config.melspec.f_min,\n",
        "        fmax=model.config.melspec.f_max,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "class HFAudioClassifierProb(HFAudioClassifier):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.resampler = T.Resample(\n",
        "            ORIGINAL_SR, TARGET_SR,\n",
        "            lowpass_filter_width=128,\n",
        "            rolloff=0.9475937167399596,\n",
        "            resampling_method=\"sinc_interp_kaiser\",\n",
        "            beta=14.769656459379492,\n",
        "        )\n",
        "    def forward(self, audio):\n",
        "        audio = self.resampler(audio)\n",
        "        logits = super().forward(audio)\n",
        "        return torch.sigmoid(logits)\n",
        "\n",
        "class LogmelFilterBankHTK(tl.LogmelFilterBank):\n",
        "    def __init__(self, sr=22050, n_fft=2048, n_mels=64, fmin=0.0, fmax=None,\n",
        "                 is_log=True, ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n",
        "        super().__init__(sr, n_fft, n_mels, fmin, fmax, is_log, ref, amin, top_db, freeze_parameters)\n",
        "        melW_librosa = librosa.filters.mel(\n",
        "            sr=sr, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, htk=True, norm=None\n",
        "        ).T\n",
        "        self.melW = nn.Parameter(torch.Tensor(melW_librosa))\n",
        "        if freeze_parameters:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "class Audio2MelspecTL(torch.nn.Module):\n",
        "    def __init__(self, n_fft, hop_length, win_length, power, sr, n_mels, fmin, fmax):\n",
        "        super().__init__()\n",
        "        self.melspec = torch.nn.Sequential(\n",
        "            tl.Spectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length, power=power),\n",
        "            LogmelFilterBankHTK(sr=sr, n_mels=n_mels, fmin=fmin, fmax=fmax, is_log=False),\n",
        "        )\n",
        "    def forward(self, audio):\n",
        "        return self.melspec(audio).squeeze(1).transpose(1, 2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    Fire(convert_to_onnx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYdjyEIUeI4R",
        "outputId": "af89124c-f818-4e33-aa8b-27cf55871d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting convert_to_onnx.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert_to_onnx.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx4OzQyieJmn",
        "outputId": "897f1bcd-f98f-4213-f554-5f865ded1cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting model awsaf49/sonics-spectttra-alpha-5s...\n",
            "Downloading model from HuggingFace...\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Exporting to onnx and saving to sonics_model.onnx...\n",
            "/content/convert_to_onnx.py:24: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter has become the default. Learn more about the new export logic: https://docs.pytorch.org/docs/stable/onnx_export.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html\n",
            "  torch.onnx.export(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/utils.py:1511: UserWarning: Provided key input for dynamic axes is not a valid input/output name\n",
            "  _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/utils.py:1511: UserWarning: Provided key output for dynamic axes is not a valid input/output name\n",
            "  _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/jit_utils.py:305: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
            "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/utils.py:714: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
            "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
            "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}